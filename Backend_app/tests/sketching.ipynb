{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84912f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bd7fb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4f57475",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = ArxivLoader(query='1706.03762', load_max_docs=1,top_k_results=1)\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(separators=['.\\n','\\n\\n'],chunk_size=5000,chunk_overlap=500)\n",
    "splitted_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a89936e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87f6648f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4813"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splitted_docs[4].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3eda81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splitted_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cccd2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langgraph_studio_run as graph_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7711e5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_graph = graph_ref.main_graph# MainGraph(model_name='llama3.2:3b')\n",
    "nodes = main_graph.nodes\n",
    "nodes.debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecb1b644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAAGvCAIAAAA4yMULAAAQAElEQVR4nOydB3zTRhvGT3Z2QvYOI4S9d9pSdhhl773K3gEKZVMoe5cCpUBZpZQCBcqGQguUVSjjA8IKhJGQQUJC9o7t77GVGCdx7NjYsWTfn/5S+XQ6n6VH77333ulkJpFICIXCc8wIhcJ/qI4pxgDVMcUYoDqmGANUxxRjgOqYYgwYg44TYrIfXE2ODU/PypKIssU5WRJGQCRiQgSEiImEkf5DNnmiRCwRCKQp0o/SHYg8MkSSl4HNiXAke5SQSES5XyQv6kNpJK8ECSPPKStOhqwCBTPLMLNkzC0EVrZCT1/r+i0dzSwI5WNg+Bs/fh+ZfWZPVMK7LLFYYmYusLQSWNqYCQSS7EwxqxtGwECy+dQpSxRLJAKG1bHst8s0TaQ5GTYFOSFG9sQIzBhxTu4pYoSMRJS3nU/HRHYb5B4uR67swrssrc1ycsRZmeLMdLEoS2JmKfAqZ91lrCehaAUvdZyeIvptZVhaqsjO0azGZw6N2jgRnnP5cNzLoJSUpGwXD8v+M8sQiobwT8dHf4x88yzVq7xNr0AfYlxkpksOfvcmKS67UWsX//aOhFJseKbjHd+8EovIqKXlifES9jTz9O4INx+LnpNKE0rx4JOO9ywNLeVg1n2isZlhpexaGFqpnl2Tri6EUgx4o+Of5r5y9bHsPt6bmAy7Fr62tjPrN51aZfUICB/YszTMxcvKpEQMhi30TUvOObUjilDUwQMd/70/NjMtp8dEL2J6DP/W9/Xj1LhIEaGohAc6Dr6T2G18OWKqVPd3OLwplFBUwnUdH1wbbu9s4eYjJKZKy75uCNFcP/GeUIqG6zqOjcxsO8jUR7kq1in1+GYioRQNp3V84UCspbXQvYw5KUFmzZp17Ngxojlt2rSJiIggeqD1ALfMdNG78GxCKQJO6xhdHBefkp5B8/jxY6I5UVFR8fHxRG8gAHftRAyhFAGn48ebp79oM9ADwwFED1y7dm3Pnj2PHj1ydXWtU6fOpEmTsNGwYUN2r52d3aVLl1JSUvbu3fvvv/++ePECe5s3bz5u3DgrKytkmDFjhlAo9PLyQiFjxozZunUreyDyrF27luiak9vfRoelj1hkzAOZHwN37XHiO+l0Mj2J+OnTp5MnT27UqNGhQ4egyGfPni1cuJDIxI2/8+fPh4ixsX///t27dw8ePHj9+vXIf/78+W3btrElmJubh8hYt25dr169kAGJcEj0IWJQtopNdiZ9sr1IuDv/OOxZikBvUYp79+7BrA4fPlwgEHh6elavXh2KLJxt0KBBAQEB5cvnWsH79+9fv349MDAQ2wzDREZG/vLLL6x51jflqtpeOUb9iiLhro6TE7IZAUP0Q926dTMyMqZMmfLJJ580a9asTJkyco9CERhdOBULFiyAwc7JyUGKs7OzfC/0XTIiBg5uQiKWkCxC6Ix7ZXC4n8fo0XevWrXqhg0b3NzcNm7c2L179/Hjx8PWFs6GvXAkkOHo0aO3b98eNmyY4l5LS0tSkjCCLEJRDnd1bGdnRsT6ssegcePG8INPnDgBzzgxMRG2mbW4cnAXHT58uG/fvtAxfA+kJCcnEwORkiBChSyoMS4C7urYu4K1SKwve3znzh14utiASe7UqdO0adOgUcTOFPNkZ2enp6e7u7uzH7Oysi5fvkwMxJvgTMaMH5O6DAJ3T42rj7lELAkLTid6AF4EwhRHjhxB0Pfhw4eIS0DQCKLBVYBwb9y4AS8CXUBfX9/jx4+Hh4cnJCQsWrQIXnVSUlJqamrhApETfxHQQGlED7x5mmpursfWie9w+ha3sBIEXU0gegCBCHgLa9aswSDc6NGjbW1t4QebmUl7vQhi3Lp1CxYaxnjZsmXoySGs1q1bN39//4kTJ+Jj69atEakoUGDp0qU7d+68ZcsWuNRED4S/SHFwoos0FAmnx0FO/BQV9Sp99DI/YvJsnBrSfqhXxbq2hKIMTtvjzqO8MtNFWSbfS796NE4gIFTEKuB6U2Vrb37ou7ABM8sWlaFdu3aZmZmF00UiERxchlHuUyKO5uiolweSMcKC0IfSXegpIiCttEp+fn47d+4kRfDwRkKF2noZ1zQauP58Xk422TIjZOJ3FYvKgCCDFj/B21uPj0gV9p5ZUlJS7OyUyxGuuTwwUoC7fyfcOBM7fk2RZ4BCuG+PzcyJV3mrHd+8KmqKDIIMhGPo9ia5eTauSTcPQlEJD0KSPQNLS8Tkwv5YYnr8tuqNi5dV7SalCEUl/Aitj1xSPvhO4pN/U4gpcXhDRHqqqM9XJrFex0fCp3VYtsx8Ube566cdHIgJsH/NG6GZoPcUKuJiwbN1sbbNfuXsYdHL2K/u7kWvCUO+nO9LKMWDf+sU/rLkTVJ8ZoNWLp925P0ym4U5teNt6JPU0hWtu4w1rUVnPhJerht7+6+E2+elz8F7+lq1G+xlbcf7iQdvX2RdPh7zLiLD0lrYfbyvixedSqEZPF7H+/qp+KCr8dkZYqE5Y2NrZuNobltKIDAj2ZniwpmFZowop+AvZYT4/Yy48Kw6RrYyt0IyIxBI2GW78yXKPuX/NoFQumR3vpOK0piC69FjPEQsYtJSclISRPiL/HaOZp93dKtYz4ZQNIfHOpZz4/T7NyHpqfE5UCrUlq1sHFsgJOJCi0sJBdJnLNSeAIlYLNNiwdgOOzBX4HAMIIsL3UdSHefPZm7BoEqwvrZOZr6Vbeu2NInOq/4wBh3rm4ULFzZs2LBTp06EwlXoVED15OTksFM6KZyFXh71UB1zH3p51EN1zH3o5VFPdna2uXmJrjFH0RSqY/VQe8x96OVRD9Ux96GXRz0ikYjqmOPQy6Me+MdUxxyHXh71UL+C+9DLox6qY+5DL496qI65D7086qE65j708qiHjoNwH6pj9VB7zH3o5VEP1TH3oZdHPVTH3IdeHvVQ/5j7UB2rh9pj7kMvjxpUr9tJ4QhUx2qgxpgX0CukBqpjXkCvkBqgY9rJ4z5Ux2qg9pgX0CukBolEwsGlwikFoDpWg1AoDA8PJxRuQ3WsBjgVBd7XS+EgVMdqoDrmBVTHaqA65gVUx2qgOuYF9BXyasCINMalMTpNKByG6lg91CRzH6pj9VAdcx/qH6uH6pj7UB2rh+qY+1Adq4fqmPtQHauH6pj7UB2rh+qY+1Adq4fqmPtQHauH6pj7UB2rh+qY+1Adq4fqmPvQ95kWSd26ddnJFewpksho0qTJpk2bCIVj0HHpIoFkGRkCGUKh0NnZeciQIYTCPaiOiwSSdXV1VUypWrWqv78/oXAPquMigWRr1aol/2hjY9OvXz9C4SRUx6oYNmyYk5MTu+3n59esWTNC4SRUx6qoWbNmw4YNsWFraztgwABC4So6jleIssi1U3FpSVnZWSiWQfFEQgRCIpY9TiE0I6IcIhBIbx+xLJAlEArEIrE0o6wmjAB7GVGOdENaLexhpI9kSMQSWWYGUQMxEoVEIisQmbGD3UuwVyz9OkZ2b0rE+TIoJkp/NsqX7sjNwwgkIrY++AYmX7aU5JSgoIfm5uYNGjWQVlIsyduFo/Ar8j4q1FNWpvTXixUeIpFVgGFrwpavWIfcQ/CLxfnOp6xuOGn5rpHQjBGLJZICOYXSYgtcTNkZQ5n5Us0thE7uFp+0dyJGhC51vH9NRHx0Bk4TTmhODnutpNeJEUokIul6lQLsEUkvDNJZIUJAEjEjy8bqXpYikqXgSIkshZHlkWWWSQEF4eKwKdJwmDQbEMoSZTrGFhEL2AzSYmRlS78UtwqbWZbOakhWH0Yikn5gpAXkFUhyjxKLJIy0GtKbklHYlVtV+ce8euZWjMjuKznSoqQ1lOtYVocP38XKOt9XsLeHIPdcyZGeRtkvVURaGYmSxNwzpoCFFSMSSe9Av1p2bQe7E6NAZzo+vCEiJVHcI7AMofCBhBjR6Z1v6rVw8G9nDIZZNzo+sDYCA15dxvoQCq84uCa0cj3bpj1cCc/RTT8v7m1ml+FUxPyjUh374DvJhP/oQMc3zyaYmTPEglB4R702TlnokfN/UQMd6Dg9JadAh5rCI9CLTUzkvZB1MN9NJMqRxs4o/EQaZOH/1aPzNk2d3Jgnz6E6pjBGMKpLdUwxBnSgY4zQEvp2OT5D/WMpspnmVMh8RTo8T/0KIg3ciMUiGnfjK7KZRITvUP/Y5EFbKiR8h+rY5BEbw3ge1bHJI6FxNxaGIRLaz+MtDPWPZUgfjee/g2W6GIU91sEvkIjFErExxCu6dg/Y88t2FRkOH9nfuu0nxNAMG9Fn/fcriK4wCnvMpzvxj6MHl69cQHRK955tIqMi2O2+fQbXrlWPmBh5DzTyGz7184KDHxOd8vZtVEJCvPzjgP5fEtND4bFaHqObfp6m3TyxWPz9hpVXr12yMLcICPiiZo06s+dOOfz7n87OLth79s8Tx08cfvUqpHz5iq1atu3Zoz888Clfjb5//y72njt3auuWvZUrVVVR/pE/Dty4ceXJk4cWlpZ1atcfMWKCj3dppC9YOEMoFHp4eO0/sOfLoWN2/7wViQMHdf388+ZLFq2FX4HvGjJ4JBLDwl6v/W7pgwf/8/byadq01fBh4ywsCj4poLSeqn94tx6th305NjEx4ec926ytrRs1/GzihOkuLtLHitLS0tatX3bv3u3k5CTfcn7t23ft1rU3e9Tr1y9XrFwQGvaqbt2GQwaNVCzw/fu4zT+ue/jofkZGRqNGn2FvmTLliKbwv5eugyZF+mg6o9mZ+P3QrydOHpk08estW/ZaW9vs2LlZWo7syfe//j67ctW3kOm+vcdHjphw6PC+TZvXIn39um3VqtVs27bjxb9vqxZxUNC9jZtW16hRZ9GiNbNmfhsf/37psnnsLnNz85evQvDf0sXrunbptXzpeiT+uvcYRKxYAuz0xEnDatWsu3bNj337Dvn7wtkNG1cV+Jai6qkaVODAgT34pUf/+PvnXYeDHt5j7yUwa05gZGT44kVrD+4/3axZAO7zJ08fIT07O3vm7Elubh67dx4aMyoQd2BcXCx7iEgkmjptzL37d6ZOmbNz+wEnR+fxE4ZGRIYTTZAQY5geowMdizXv5/157mSzpq1aNG/tYO8wcMAwG1tb+a7Tp4/Wrl1vyuRZTk7O9es1GjZ07NGjB6HF4hdevXqtXTsOoth6dRs2avhpn96DYJgTkxKJLLTy9m3ktwtWNW7czNGxyOeEIUpLKysYTlSgS+eeI4aPh/4K5NG6nj4+ZQYNHF7KrhTMMOzxs2dPkHjj5jXcfl9Pm1+tag0HB0dUvlaturDZ2HX5yoWYmOgJ46d5eHj6+voFTpqRkpL7RB0OQbsxZ/biT/wboykbN3aKvYPj4cP7iCawKx3wHQO4+NA9GsoaNWrLU5o1DZDvQhOJqyvfVa9eIyQ+CPofKTbwHGDYZs+Z3KlL85YBDefMm4rEhDyFlStb3srKSnUJL18+r1SpKsphP37RrvPkwJkF1SNOBgAAEABJREFUfoLW9axcuZp8u1Qp+9TUFGzAOUGtypev8CFbpWpsfyAi4g12eXp6selQv7u7B7sNc44bDHcR+xF3ad06De4/uEs0wTjWDdZF/Fi2Bknx88MRlEgkNjYfbDAsELuRlZWFZhRuButpyNHIHl+79s+8b6bBpI0ZPblChUq379ycMXOifC88ZrUlQFsqrPVH1lOpDw1XwcrKWjHFxsYmPT0NG0lJiXC9FHdZWubehzDMqAbuVcW9qmuurD7ECOyxDnQsyV2xp7iw5hAXQJ4SHx8n34Xr17ZNRziIiod4e5Umxebk6T/QKMNnZT/KW+HiY2trl5qWqiKDTuqZ/xttMzLSFVNQAVcXN2zY2zuwgpaTllc32GZ0Fpcu+U5xr1Cg8aCUEQzG6kDH6LUwmsw/NjMzQ8v4+vULecq16//ItytUqJyckgzXlv0IuUdFRchb0uIAA+bp4SX/eOXKBaIhVapUP3HycE5ODqqKj39f+PPMmWMrV2xUzPPx9cz3jZWrI+DwPCS4UsUqbAp8el+Zm4Hfgl0vX4b4+VXEx5CQZ7Gx7+R1SE9Pd3f3ZKMxALFwRwfN7DH6eUYw/9gw/bzGnzU7d/7Urds34GAgdoFIk3zXqBETr127dPrMMRSLfsyixbO/mj4W7TiR9ZBwde/+75bq5rtihcoo+X/3bkOIKJxNfBsdVThnmbK++Hvp0vnHTx4qpnfs0A3fuO67ZfBJrly9+NP2jS6ubnJ3WW09tcDfv7G3d+l165Y+DX6MUBrcFfzSvr0HY1fjxs0R8luzbgnUDAUvWjIbFpo9qkF9fxy4Zs3i6Oi3iOUdPfb72HGDz549rsk3G8n8Y8PciUOHjK5Vqx7c1sFDuoeGvurVU7okq5mZNCYAl2Dbll8RuMVI2/QZ4+GqLlm8zlLm1Hbu2APO5dczJrx4+VxF4cOHj0f/fd78r9p+8RkuMEJvVatUnzU7EJGyAjlhxtCH27V7y08/5bO1pUuXXbF8A0K5+C7E7D7x/xxR3gLHqqinFsDwI/YHgSJwNmBQlzt3/1u8aA2+Arvs7OyWLV0vyslBt/XL4b1wrsqVKy8/EKHD5s1bQ9yITB/5Y3/r1u179NB8sXH+OxY6WN/t74PRT/9LGTK/QvEPgWmJiXlbVmYOAWKiv/6688TxS4RS4uxe+HzoHD97N35P9dKBPZauqaphjxfCHT124OEj+9EaXrh47uDve7t06UUohsAowhU6GZcWa3wivhw6OjEx/ty5k3A9MVLVvVtfhMmKfzgGsR8G3VO6q0OHbhgOIAYCjvKcuUV++95fjsojjFyCYfjvV+gq7kY0pcDIgkZM/2peVrbyHpVN/lBrCSN1mrcVOZzGSRHnWxOfv/DyuSZ2Yg038fL0JrxCuow+4T26iB8zdB0WioHRhV9hDP6V6SIxiuunCx1LXx5EKDyFMYpXjNPn/inG0Jzq6HlpQuEvtJ8nQyB97p8qma/I3rBJ+I4u3qsgpusU8hipBaLrsFAoXIDqmGIM6EDHFhYW5pZGsJSHiSIwEzBC3q9rpgP9efvZiEW0n8dL3kdlCRhi70z4jg50XKGOlUBIHl1PIhS+cetcrK2DOeE/uvEHPu/oce9iLKHwirAnWbHhGYPnliX8R2eDkolxon0rQ119LH2rl7K0EYhyPsRy2JnaivO1GYEAo9mkwCTuvA8Y8BdgqJvJl5j/eOk+iaz2inVg2GOlS6znpqMQRqKwW5amuFpzXqnSsRzpqZD9T75XtqSqwinKP+ecURjSzXec9Eul/wrNh2Tkb12UH8uwzyBI8v8+aa0lCMyL5XNiP1SU5F61DwfIttjBKPYnyI6SlSytVm7GvCqaCc2S32e/fJycmpA1ZoUfMQp0Obie+Jac+jksOSEnJ0tS+MlTicJjYPnVIk8t9kRYJq/EovZKlG2zBykqu5hfVMR3ybSal6PwT8qVk4T5cAOpmheYb2/erS+R5B7+oZzCv4jk/wK5OWBncEnytJ2H0IwRmgtcPC16BvoQY8EoJokUzYgRIwIDA+vUqUMMx8uXL2fNmnXw4EFC0RvGrOOgoCBvb28XFxdiaCIjIyMiIho1akQo+sFodRweHm5nZ+foyJVHiTIzM3Gq1S4tR9EO4xy/WLFixY0bN7gjYiJdlM1y+fLlp06dIhQ9YIT2OCoqSigUuru7E+5x/vz5+vXrc8HVMTKMTcehoaFisbh8+fKEq2RlZRVe2p7ykRiVX3FQBpdFDNLT01u3bk0oOsV47HFqair6Us7OPJgrEBMTc/Pmzc6dOxOKjjASHSOwFRYW9umnnxKKSWIMfsXt27cXLVrEOxFv2rRp7969hKILeG+P0avLzs7WesFWw3L8+PGaNWv6+RnJJAcDwm8dJyUlnT59ul8/zVf8pRgXPPYrEhISBg8ebAQiDggIMO5ZLiWAkc8T4gXx8fEHDhwYO3YsoWgLX3W8atWq6dOns69ApVB4qQO4E2PGjDEyER85cmT37t2EohXUr+AQCF/4+Pg0aNCAUDSEZzreuHFjjx49cLEJhaIAn5rm1atXo2tv9CKGy5SYmEgomkD9Cs6RmZmJ4cmlS5cSSrHhh4537txZtWrVxo0bEwpFGTzwK44ePVqlShVTE/G///67bds2Qike1K/gLseOHbOwsGjfvj2hqIPTOkZINTU1FdFiQqGohLvrxt69e9fW1hZRNmIg0tLSRCIRMTTXr1+vVatWqVKlCJ/BpdTruBX1K4okPj6eCzomshlRDg4OvH4Ni6urfl/dycV+3s2bN2fMmEEoeTg6OtJ3CamGczqOlrFq1SpCUSAnJwd+DqEUAbf8ClQGF8zcnBMr8nLHr2DJyMgQi8U2NoZ8EbzWmJBfATPcqVMnjohY3yAo3rFjR9V5Xr169cUXXzx8+JD9aGVlpSsRr1y5ctq0aUR39OnTZ9++fcRwcEXHMMPomNNloxRB327AgAFubm6KiRyJonANrsTd3r171717d0JRwNnZeciQIQUSYZIRvrC3t6fPECjCiXPRrFkz2B7CeR4/fjx37txevXqNGDECg8ZsxwstCT4uXrxYnm3WrFnjx49H+uHDh/v27Yt2pn///hiWGz58+F9//VW42NevX//www+jRo3q0qXLxIkTT548yaYr+hXHjx9HIW/evBkzZky/fv0mTJhw7ty54tQZwR/cDB06dEDJf/75pzzdzMzswYMHAwcOhC8XGBj49OlTNr1bt26///67PNu6detwILuN34K6wX9AaYjrL126NC4urvA3oliUeeLECVKCGF7HV69ePX/+PPe7LxEREXPmzEFn67vvvvvmm28gsq+//hpihSDga167dg0DN8h25coVKG/mzJlIFwqFGI+8ePHizp07Dx482KJFi7Vr14aHhxcoeevWrXfu3IE0cTNAuND0f//9VyAPug0pKSmbN2+eMmXKmTNnmjRpgmrExMSorjNEvGjRoi+//BIlf/755zgElWF34ViIEvFN7MrOzsYutT1+/KJDhw6hHcBv+emnnx49elR4/Y2wsLCFCxfC9S/h1ZIMrOPnz583atSIF6tPQAG4kFBwmTJlypUrBz29ePECtha7qlevDgu0ceNGWGjYaQykIwN7FITetWtXa2trDMghHbfrpUuXCpQ8e/bsZcuW1a1bt06dOiinUqVKt2/fLlwBqA3ms1q1aoglt2nTBrKTdwGLYs+ePZBvq1atGjRoAHOOlkQevIuNjWVX6q9Xrx5qGBoampSk/o1b3t7eaA3s7OxcXFxQJi6f4l6YZ/yWmjVrotEgJYsh/WO0s1ADX5ZQgVNRpUoVuf/j4eHh5eUFJcEpIrLfAk1DGQgw9e7dW/FA6JLdgP5wCCxWgZKhyGPHjt26dUtuqj09PZXWARVgN6Ak/FXd4UOQDo0GRCxPGTlypHzbz8+PLQTA2yayec9EHfLfAnBnyu8K/DQcPm/ePBSFVqvkfXeD6RjG7Pvvv+fRtAE068+ePUO7r5iIGDO7AUML73b37t1wRgtcRcUbFdsFhjOgNth42Nphw4bBOkJbKiJihUf14OfA5RAqeyEpG28uykygbVFRrKbgVkRnAI0PmguDrIprMB1XqFCB8ApED2rUqFEggMBaMpCYmAibCtsM3xEmUNGgQrhy7x9Gy8nJSbGEkJCQ4ODg5cuXo31nU3DDFH+hbxQolFF4FxSMOwoOOvkIxGJxMXNWrFgRjdL8+fN//fXXkp+iaDD/GF0fpZ13zlK+fHkEB2vVqlUnD0dHR/jK7N4tW7aULVsWTSra6w0bNigeeO/ePXYDmoPnIHedWdhH8eTDXaEySLFhxap0F8RduXJl9MbkKbt27UKfUmV50reFp6enyz8W7pUWhb+/f+3ateG6IKDx5MkTUrIYTMfoLyMOSvgDIk0wTtAr2mtc3R07dowdOxYhM+xCeAFRl8mTJ2N76tSpCDwhAsMeBZHBTiNeBl8WvS5IuWXLlorFQtZsHCA5ORnZfvzxR/Sf1AYi5GCQT1j0W84RN0AkBIXfv38f0Qm0Fb6+vkQlVatWxW9hrfhvv/2G7iDRBIQp0HFHt7WEZ4MYTMdwB/m1LDtceYgYupk0aRKsDsSKTioaU1xyBFnRt0NfHtlgoRGCRViKNbRwPXv27IkwHCSF0Ur4vqVLl1Ys1t3dHcEvhG9RwoIFCxAjQ058RDi5OLXCjaGit4ewBmLbMJCoAP6i3W/Xrh1RCW5OeD6oMyInuGML3HXFYfr06XCUcU5ICULnHxfJx88TOnr0KMJwp0+fJnoDwTJLGYTbGO08Id75x9wEClbhV5gOBotXwAWUd/YpWjNgwICidsGHMZ2HzA3mV0RHR8OWcOpVjQXg2vxjpWBUpai4G84td16fqm+/wmD2GONhhPLRoE+GcRD6ul/qH/MbtGmKI3MmC/WP+Q19NSoL9Y+LpPhDsgYEQzBwLRSn73ATRgbRG9Q/LhJePHDxzz//YCRcPg/OZKH+Mb9p2rRp9erViclD/WN+Q9fSZaH+Mb+5efMmgm516tQhpg31j/nNrVu37OzsqI4NpmP4x2XLluXXlDcO8umnn9IFAAj1j/lOw4YNCYX6x3znf//7X05OTqNGjYhpQ/1jfnPv3r20tDSqY+of85v69esX53l9o4f6x/yGRipYqH/Mbx4+fJiUlERHQwwWsoF/TEX88Tx+/Pjq1avE5KH+Mb+pVauWl5cXMXmof8xvqlWrRijUP+Y7wcHBOJPsWommDPWP+c3z588vXLhATB6D2WPqH38MHTp0gBkmsoUu2RSGYZycnEx2Sjdd342X9O/f38rKCtoV5CEWi+Urdpog1D/mJdnZ2YMGDXrx4oU8xd3dfcmSJRjeIyYJ9Y95ibm5eZ8+fRSXdatcubLJipjQ5/P4S8+ePeVLKbu6ukLWxISh/jGPGThwoK2tLZG9tsPEh6ZN2j8Oe5KZkZopVjwBDEPkJ4RBOAB/GAnJf4qQJGYkAlJkTqI/lB4AABAASURBVOkWuy3b8eG43L/51sVgv5HNKM+et6FYpiyDNLNilbZt3fruXWzPXj2rVq2aL78sgiG9uOwhSGUUK/shg/yQfHX98I2yahXWCLsahYTkOzkChoglBXJJPxfQmOJJxkHmZhWq2AitycdgousfH94Y+e5NBi5QTpa6xVbYy59P67nCy5dH2VlkNVIwUZasJmsRBcqLKFiA6vwyISlZBYXJFaKS+hSjTNl5KOLYDzVVk4FI37gjQDYLa7NOw708fLVcHskU48eH10cmJ+Y06eHpUZYuKsUVbhyPCwlKHPR1uVJu2iznbHL+8d5lb7IyxT2nlKUi5hSfdnEZNNfv19WvE99pY1hN6/0gYU/TUxKzOo0tTSicxL2szfGtb4jmmFb8+N4/idY25oTCVap96pyanEM0x7Tix6nJ2YyQvteHuzi6mYlytFnm1LTmHyM6kZNFdcxdELUTa/UqC4PpGP4xX96QTilRtFolma5fQeEQWreVpuUfCwQMQxdD4zBar1hvWv6xWCyR8OBlCSaMtgaZ+scULqGtQab+MYVr8Go8zyD+McMQfb40iPLR5E680xjT8o8lEmKS0/v4g7Z21cT8Y2qMuQ0j7YVrY2lMyz9mCPUrOI1ssr82V8i0/GPqV3AdCdFuQjx9Pq/kePkypGVAw6Cge6qzLVg4Y9r0cYQ/DBvRZ/33K4hOYIh2r+81Lf+YF05Fs2YB2dlZxFTRrr00Lf+YF05FQKt2xITRztTQ9SvUkJOTs3XbBjSdHTs3mzk78MaN3EWzz58/HdDGPyTkGfvx8ZOH8BkuX5EuGdipS/N9v+2Ge4AUbM+eOyU5JblwyUf+ODBj5sTOXVr07N1u0eLZEZHhbLrcr3j16gVKePL00fxvpmOjT78OP25ZLxLlzmt8/z5uydK5/QZ06taj9dLl89+8CWXTWe8F9ezV54uRo/ur+Gmqy09LS1uybB4Kade+8Zixg44e+11+4OvXL8eOG9y+YxP8tCdPHiqW+ejRA/yoLl1bDh7aY/OP36WmppISwbT8Yy38ig0bVx06vK97t777fj3RvFnAgm9n/HP5b6S3adOhQX3/teuWENligdhoHfBFs6at8FEoNPv90K+dOvW48NetVSs2hYW93rhpdYFi4SUjsUaNOosWrZk189v4+PdLl80rkMfcXProCkoOCPji3Nl/585ecvD3vRcvnUci1DZ12ph79+9MnTJn5/YDTo7O4ycMZe8E9qg9e7f37TN42lfzVPw0FeWDWXMCIyPDFy9ae3D/abg6329YCcUT2ZJcM2dPcnPz2L3z0JhRgfsP7ImLi2UPCY94M33G+IzMjE0bdy3+ds3Ll8+nfjUahoAUH20bTNN6Pk9TvyIzM/PPcycH9P+yS+eeDvYOHdp3DWj1xZ5ffmL3QiWvXr84feYYbBWs4+TAWfIDK1ao3Kjhp+iyVK9eq2uXXpcuncflVywZ6bt2HBw4YFi9ug2Rs0/vQTBsiUmJhevQvFnrFs1bQ3N16tT39vJ59uwJkd0GuD3mzF78iX9jZ2eXcWOn2Ds4Hj68j+QtLIEye/caWK1qDXU/UXn5N25ew1d8PW0+SnBwcEQ9a9Wq+/OebdiFNicmJnrC+GkeHp6+vn6Bk2ak5LU2f/11xtzMHAouW9YXu6ZPm/88JPjqtUuk+ND5FfogJCQ4KyurUcPP5Cl16zQ4c/Y4BAdZ40IOHzZu208bRTk5c+cutbOzk2erWLGKfNvHuwxEHJnnNrAIhUKk/LB57ZOnD+WNb0L8exRboA6VK39Ycd7OrhQrmqCH96C8+vVyX5sH7aJi9x/c/XBUpeKuU6+0/FevQqysrMqXr6BY4N8XzmIjIuINdnl65r7MwcXF1d0991I+enS/qkz37Efk8fYu/SDof7hPSDHh3Xw3g6xfIRSSHKEGtzzr106aPKJAevz7OFZwPbr32/3zVjOhWe1a+dZstbS0km9bWUuXyklNTbGy+rBmzrVr/8z7Zhrs3JjRkytUqHT7zk24lUrroPT10VAb7g04tYqJjo5O8m2LYseClJYPV0GxtsDGxiY9PQ0bSUmJ1tY2irvkPxa1ehr8uECtcK5I8eGdPTbI/Ar0YSQiDW55GBsi9R/m+viUUUx3d/dkN+Adenn5QFLbftowZfIHvwKqlW9npKfjbwFZnDz9B1rqkSMmsB9TlHUEVVfM2tp66ZLvFBOFAm1WMFGKra1tRka6YkpqWqqrixs27O0dWEHLSUvLbU+cXVzxo4Z9OVZxr4N9STwVT+cfqwIuAVtJeLFsCjpk6NXBOBFZtx0u44bvd+RkZwdOGdm2TUd4vWy2+/fvyAuBj2hmZoY7AS2yPBFWzdPjw3uWrlzR7N0IFSpUTk9Px+3k4527FkdkVISjgxPREVUqV8/IyEDNK+U5SHDffWVuBqqNXYiK+PlVJFLX61ls7LvcWvlVOnf+VJ3a9eU2HqeodOmypPhItDTIJrb+sYZnCXr9cugYdOzQ6YGjjEgF+uPs2JVYLF6ybG7rgPboCcEIIei7bMU38r75u9gYhCwQVUBv7OSpIy1bti1w06IjeOv2jf/du41DkJNNfBsdVcyKIVTi7994zZrF0dFvExMT0NFEIOzs2eNER6BwuLbr1i2Fn4Au7I6dm6Hjvr0HY1fjxs0tLCzWrFsCNUPBi5bMts/z6Xv1GojTsmnzWuxCHBDxyuEj+758FaLBFzNaOsgm9v48zc9Sv75DYPz27d999+5/trZ2NarXnjZNGsz6dd+u6LdR69ZuZbNNnDB94OCuv+zdzraqnTp2RyQVAVRsozc2aeLXBYodPnw8muN587+CWYWTjdBbVFTErNmBc+csKWbFli9df/zEYcjo8eOgMmXKtW7dvkePfkRHoAFZsmjtlq3rEc6Dav38Ki1etAa3K5H2Be2WLV2/bdsGhMbR4Rs9KvCvv8+wR9mXst+x/cD+/T+PGTcINzD6fF9Pn1+5UlWifwy2TuGKFSsqVqzYq1cvUoLsXRGWnSHuNdWX6JOu3QN69ug/ZPBIQtGQxHjRH+tfTVpfUcPjTMw/ltDnTLmNttM2TSx+LHvu34QmbsKtnzN3SlF79/5yVB7r5Tum5R+XzHP/x/74m3ADeLTbtu0rai8HRSxbx18bTCt+bGr2GHh5ehP+ABlrF3gzMf9YYpqvkeAPdH5FsWByX7xC4Sjaen0m9nwejVdwHDq/ojhI5zRSe2yMmNr8CgkfntCjaIzJPZ9H+3lcRkKIhK7vpha6/jHHYWRP/hPNoesfU4wBOv+YYgyYln9sbiEUa/c+IEqJIBAQgZCu76YOO0ch9Su4TNybbDOhNpo0rfUrPmnjkp5C7TF3eXI73s5RGx/BtNavcCtn4eRm+cf32rzBmKJ3RCQ2Im3gjDJaHMqY4MSZU9vfvg3LrN3EueonpQiFAyTGSW6fi4l8kTJyUQULa21KMJiODfN8Xh6nd0aHP0/NyZaIRGItuhWS3BW2NDt1Eg2nD0g0n24gJoiPF7dWGpUvkWgwxUp2foqbGR07RiCwKWU2aE5ZobYrF5hW/FhOh+HSaElWOklPF5H8DjMjl6d8iymkWFmwXrkFYKUhUVLmhIkTlq9YZm/3YcUg6XRbdqS8wFcweSqT5N+j8EExPTT09Q+bf1i1crXSqhb5KwS5A2hKfgRT6Nfl/S62mEOHDv1+6KCAEQoFAksrSzMzMxxibWXt4uo2b948hp0AUOgXMcq+DWXYO5OPxGD2ODo6GvFjAzz6bwiys7P/++8/T0/PChUqED2AkzlixIiTJ0+SEmT8+PH4UfKPsrndErFYfO/ePVLimKJ/XMJcuXKlXLlyZcqUYfS5jHh6erq1tVaupbY8e/bsq6++evv2rWIihgVOnTpFShy6/rF+CQ0NPXLkCHoCjJ7Xwi9hERPpAoeVv/jiC8Xl4eBdGETEhL4fRK8kJiZmZGR89913RP907twZX0dKlokTJ/r4+MCXIDK/YseOHUS65FcSKXFMK35ckowaNcrc3LxKlSqkRBAKhcnJmi12qBMmT57MdnLQa69RQ7rccrdu3S5dukRKFuof6x6cUjSvMFT16tUjJQW6krKggQGeE5g2bRpcxPv378tTjh8/3qVLl6ioKC8vL1IimGj8WH8EBQX5+flBUiY1m69Xr16IxBVI/PnnnyMjI2fPnk30D/WPdcnLly/XrVtna2tb8iJG1PbixYvEQBQWMRg6dCj6grjQJeAxU/9YZ6C7gyDUrl27iCGwsbGJj48nHKNnz57u7u5paWljxozRq5qpf6wb4COuWbOGMdyLJtn3hQmFOluSXrfcuXMnODh4wIABRD9Q/1gH7N+/Hx2a5s2bE4o6pk6dCn+jbt26RKdQ//ijiIiIwF8MBxhcxEePHl29ejXhPPDjcduTvAZEV1D/WHtevHgB60KkL0oy/CwR+Me8sAsuLi4rVkhfTHH69OmDBw8SHUH9Y+2BCUTMn3ADdpqO0peIcZZVq1YFBAQ0aNCAfDR0foU2rF+/nsgGrghnYKSTeHm2NseMGTPY8U72fH4M1D/WGETWqlYtiXe3aERoaGi/fjp7z02Jwb4E1tPTE5omHwGdf6wBKSkpOO9hYWGItBCOERcXN3DgwLNnzxJ+kpmZCT1gPAU9Zjc3Nw2PNrX3530EMHjjx4/HBgdFTGT9J0PNmdQJ7AgofOUhQ4bAXhANofHj4rJ582ZWxxR9k5yc/P79+9jY2OJ3Aal/rB42PMR9EXfs2BHeGuE/pUqV8vHx2bZtW/EjATR+rIYtW7bY2toSPoDLr0WLzE3MzMy2bt1aurT07dmXL19Wm5/Gj9UQFBRUq1YtQjEce/bsuX379oYNG1TkofFj5cDtGTtW+qZoKmKDg57fuHHjsPH48eOi8hhMx9kyCCeRvrR+0yZ4FIRXwGJduHCBGCPVqlUjMmdj1qxZSjMYbB0WBDtTU1MJJ8HA2KJFiwjfQAc/IyODGC/ly5cvKrRM/eN8REVFTZo0SenTDRQuY8gR+VGjRkVGRhLOAGN25coVKmLOkpOTc+TIEaW7DKljPz+/GzduEG6AdiktLa1Pnz6Et3zzzTenT58mxgs6VEUtBmIw/5jIpjvpdjK11mRlZbVo0eL69euEwmHQz+vZs6fSXQb2j6FjLjxSdu7cubZt2xIKbzHwjNV+/fq9evWKGJRHjx61adOGUDgPR/1j0LRp04cPHxLDERAQgMFPAz7nrEOof2wwAgMDieF4+vTpn3/+Ca+LUPgAd/1jjJwhZOvj40NKnKtXrzZo0KDkl1ul6AMD+xUYOZs2bVpISAgpWfr27evl5UVFzC+46x8T2azZEu7qJSYm7tu3T0+vODAg1D82JIMHDyYlyJkzZ+BOODg4EArf4K5/TGSNxYMHD+rXr0/0z+TJk3HbNGzYkFCMC8P7FbjJVq9e/fz5c6J/vv/+eypi/sJp/xgMGDBA3w+W/fHHH8HBwcSoof6xgempUGFyAAAQAElEQVTcuTPRJxs3bqxTp06JvaqDoic47R8TWXuBkeHU1FRsoK5Tp07t378/0ZYDBw6sXbtW8RWFFKPHwH5FkyZN0MPz9/dPTk7GmAjCyU5OTuxDLFpz4cIF3A+sHwx3yoBvG6DoFu76xwjiFlgl0tbWtmLFikRbIiMjIyIi2ALr1atnZ2fXsmVLYhqYsn9sYB1v2rSpUqVK8o8wybDH7Np12nHnzp3379+z20KhsGReFkQpGTjtH798+fKrr74KDw8nsocyBg4ciI9EW6ZPnw6/QtHA4964e/cuoRg1ho+7+fn5TZkyhX0O1tra+mPeHJGQkPDs2TO5iHFXwLRXrlz5I9ckpXAEFf4xJ+JuLVq0iIqK2r59O3QMWRNtgd2FlGGALS0tfWQEBAR89tlnWqxDykfgH3/66acdOnQgRgrrH/fo0aPwro/QcTb5c3905MvMTGm4TEwk+EcYospLYfcx+VKYvEMadq/dEFsnvs9hSL6xPbGECJjCRTHKvqts7wY7FL8i7CL+S2CYJOlEeekBklJO5uWr2TXt4UIofEPH/vH9y8m3/4rLSBUxZgJLGws7BytrR0tLW3OJkBEoPDYqKSBZRqY+hVTpFzOyFFIwRZ6rsPSVZstNl6m7MEJCUK+MlKykuJT0xKzsjByxSGzvbN5usJdHOQtC4T+a6Tgri/yy5HVmutjWybpcPXfCW7LSxeEP3qYlZcI8D51fjlD4ANr948ePK/UrNNDxxQNxj27Gl3K1K1fPeNzNkOvhGanZ7Yd6V6hjQ3iO0fvH6enpbdu2vXLlSuFdxfWPT/wUFf4svWab8sS4qNi4dEpcxtk9US16udX4zJ5QOMzH+sfn9saE3E+u3sqXGC+P/g5t2tW1dlMqZV6iPn58YtvbV4/SjFvEoEZAuavH3937J5FQuIr28yseXkkOC06p0qwMMQGqN/e9cvQd4S10fkWRXD4WU7qGJzERhMTB3XbbHAOvb0QpCi3949/Xh8fHiip/XpqYEo8vhtZq7NC0Gx0o4ROq7HHMmwzfet7ExHAp7fDoX+olcxFt/OPjW6PMLIQWNhx99fa9oL+mz/8kJTWe6BqPSo5iMfPgShLhG9Q/VkLkq/RSbvx4b5zOsbQxe3CFl+9wN25U+MdFjIOISE6W2LuaifqIDl72MSGxhG/w8d08GmFubj5lyhSlu5Tr+Mb5eL2upPo67MG5i9vfhD+2s3WqVqVJ25Yjrayktv/ajd/P/7Nz3PAf9+yfHR3z0sujYrPG/RvV78QedfLsxtv3T1ta2NSr3c7dVY8vK3ctZxcV/C4jWWxViqNulWmiYn6F8usUE5ohNNfXMvGxcW+27p6UnZ05cfT2oQNWRkU//3HnOJEoB7uEZubp6clHT63p023O6kU3atdsdfDokviEt9h1/b/D1/871KPj15PH7HJx8j5/cQfRJ4yACb6XTHgF9Y8LkpyQTfRmkO/eP2smNP+y/0oPN19Pd7/eXedGRAU/fPIPu1ckym7TcmS5MrUYhmlYtyPCghFRz5B+9d+DtWsEQNk2Nvaw0BX99LssEH59XHgWoXAJFf6xch3LnmEmegJORZnS1W1tHdmPzk5eLs6lX4Xek2co61OD3bCxls52SM9IRn1i37/xcP8wS6m0d1WiT3AXZWaICa8YMGBAgwYNiPGisX+MVpXo7fHT9IyUNxGPETVTTExKjvvw7YWagozMVLFYZGn5YWqlhYXely42tyL8Yt++fcY9b1OFf6xcxza2ZqkJ+mpVS5VyKV+ubrtWoxUTbW1VLeRqZWkrEAizsz+8dDYzK43oE9xK9s6WhFdUrVrVw8ODGC8aP5/n5GHxNkxfbyr29qh05/5pP9968geb38a8dHNRFX+AhXZy9HodFtT889yUJ8HXiD6RiIlvVZ7NrIdfQYwajf3j6p/Yi3P05VgglCYWi4+f+S4rKyPmXejJPzet3TQgKlrNqxXq1Gwd9PgihvGwfeHKntBwPb7lKTkObZHEnW+P7j19+lTfy5YaFhX+sXIdu5exEJoxcaF6CTwh4DB94j4Lc+v1W4au2tDn5eu7vbvNVdtva9182CcNuh49vRaONYxxl/bS36OnRWTiQuOtbA3/dkpNgX98584dYryomF9R5Hy3A+vCkxMkFT8zuXlC4Ok/byrXs23Vl2ePIULHVapUMeKQhYrn84qMrrXu656ZZooB1OTYDHGOiHciJiYQd9Ny/vHuRWGMwKxcA+Vd4ITE6DWblHcsrC3t0jNTlO7ydPObOPonojvmLQ0oahfGCIVCJR1Z3zK1Rg5ZX9RRz6+Gu3qbd5/Av4YI/rGTk5NxhyyKQpWOM1LJjm9CarRW/ow0VJKYFKN0FzpwFhbKo68CgZmjgy4XvngfH1nUrqzsTAtzJbEzM6GFvb2r0kMSIlKjgt+NW83LV5IZ/XP/GsePWaxsSZlKtsGXw6s0U/JICEyds5PhjZZu6xD57N1n7fm6Oocpx4/VjD53GetlYSEJvRtDTICQ6xEeZazrteLro//UP1bDllkvza0tKvh7EePlyaU3dg6CwXP0OB1U35iyf1ys2UBjV/iJs7Nf/hdJjJTgy2+8y1vxWsTEtOPHxZ3VNmpJeQEjfnIpND2ZZ7PAVBP3OuXR369dvS26juX98gYm4h8r3aXZept//RYdfCfZwtq8dHUva0f+jXgpEh+eGhPyXiwWNenhXuuzUoTCeaDjH374QenQtDbrH2OoLzYyUyBkrOwsnLwdHb31PoVSh8S+Tkp4m5KZmsUQUqaybefRxrPKDI0fa8OF/e9eP0nNTBPl5IjZGcP4q3VpBY+VrjYv+ZDI5K7ozQgYiViS/0Ci9juFZgIcJRZLF/5mhMTWzrzaJw6ftHckxoUpx4918L6m9ETR62fpibFZ2dliIs7znuX6YvLWj5fPzM8nvbzdigLNfQmCGCZfLJIoHiKAky6WKCn/w0f5avcfsgmEQgtLgZObRcV6xrySgSnPr9DBe26sHYTVGmn/xjuKrjDl+ceceL80RSfQ+DHFGKDxY4oxYMrxY068B5KiE6h/TDEGqH9MMQaof0wxBqh/TDEGqH9MMQaof0wxBqh/TDEGqH9MMQaof0wxBqh/TDEGqH9MMQaof0wxBqh/TDEGqH9MMQaof0wxBqh/TDEGqH9MMQaof0wxBkzZP6Z+RbGQyCDcplatWp6enmIx1xfgY2QQzdH4/XmUAqSkpGRmZhJu07JlS/x9//494TbOzs7a6Zj6xx9LcnIy93WMZlcgg3Ab6FjnlaT+sfGQnp6elWXMr9ii8WOTAM2uUMjvxXxVQ+PHJoG1NZ8W8NUCjd8vTSlJjh492rFjR7XZ+vTpg8iaigxodrUIVqxcuXLatGlEd6itp9Zo/H5pCgeBKapZs6aKDNQ/pvCAvn371q5dW0UGU/aPqY61BGHaFStWDBkyBPJatWpVeHg4m75+/frBgwdnZGSwH/fv39+tW7e3b9+GhIR88cUXV69eHTduHDYGDBiwdevWwsW+fv36hx9+GDVqVJcuXSZOnHjy5En5Lnl7ffz48f79+79582bMmDEoCgWeO3eOyPxjtLwq6nzz5k1UuEOHDij5zz//lKfjBnjw4MHAgQM7deoUGBiI8W02/RsZ8mznz5/H16WlpRHZTYW6oT4oDQMTS5cujYuLK/yNKBZlnjhxgugC6h/rGJFINHPmTFykSZMm/fjjj46OjpMnT46MlL6XbeTIkWj+WMHh0kLHUBuG2VhL+dtvvy1YsABCHDt2LHRw9uzZAiVD3BhbnjBhwuLFiyEaaPq///4rkAdixbjM5s2b4SyeOXOmadOmsFIxMTGq/WOIeNGiRV9++SVK/vzzz3HIxYsX2V04FpWZMWMGdrE2T+2oAiR16NAhhIEPHjz4008/PXr0aO/evQXyhIWFLVy4EK5/586diS6g/rGOwWWDOcSFb9SoEaL6MJ/29vbormGXnZ0dDCTcOMh6y5YtVapUad++vfzAJk2aQNMWFhbNmjVr0KCBXElyZs+evWzZsrp169apUweWrFKlSrdv3y5cAagN5rNatWoYGGvdujVk9+LFC9X+8Z49eyDfVq1a4Xthznv16sVaVhAbGwszjG+sV69e165dQ0NDk5KSiDq8vb379euH3+vi4oIynz9/rrgX9zB+Cxx63MZER9D5FToGOoZtgNrYjxATPNegoCD2IzT6119/we5CH9u3b1c8sEKFD29ghw4K6xiKPHbs2K1bt+SOCnSvrAoEdwi7ASUR2ci5Cv8YdvrVq1cQsTwF7YZ828/Pjy0E4IbE3+IMXuIek2+XKlVKflfgbODwefPmoag5c+bocOgOzeCpU6fgyVhZWRXYRXWsDRANLCLafcVEeBfybbiPCGZB3LBVinkUL4ClpWVqaqriXqgN/ihKHjZsGKwjtKUiIlZ4igIbP8bhhb1k+OsoHN+otCjcACqK1RTciocPH4btRHOBlofojvv376OxKixiQnWsHfAlcDa//fZbxURFW/jLL780btwYru3ly5dhnuXpuAHk2zBaBS4J+oLBwcHLly9H+y7PX+BOUAuMFqRsY2OjmAgFwy4WuG00pfjB6YoVKw4fPnz+/Pm//vorer1EF+BUwEsp8LvkUP9YG9AQw8K5ubnVycPd3R2J7F70vdCIT58+HREG9AIVtYuuoXwbHq2vr69isYmJifjr6urKfgyVQTQE90Zhm4p7rHLlynCH5Cm7du1SGjBRBNZU7i0AuaujFn9/f7RFcF3Q333y5An5aNCbRL+2KBETqmPtgL1s2LAhQmzo6UN8iCuhn4SwFJH1maAPXEKcdHSDoCpFuSAWAd8XG9evX0crqeiwgnLlyrFxgOTkZPQjcQ+g/4SvIBrCOhiKEgSIG+DbUTi+F9EJKKPAXVQYuODPnj3DPYntu3fvos5EExCmQD8YnkCBmmhKdHS0j48PetUq8lAdawliWAh4wQeAK4yeWcuWLdHTR/rq1athmNu0aUNk9gzxOOhbboZhoXfv3g3HesmSJchfwMOGUcfVQvi2d+/e6CYiRgbx4SPiIURzcAspxnRRpREjRsBAImKIv2j327Vrp7oECLFFixYINqOeiDfjtiQagkYJjvK6deuItsCZwW2JSIvqbHT+cbH4+PnHsGqIx61Zs0b12LI+gMfMqXE+jeYfw6KzLZhqqD02ftDtQ2iZ8BA4bJcuXSpOThqvMB4Qs1PsySkSEBAwfvx4wivQACJUXMyWhPoVxYIXzzXBG4bpVboLLqaDgwNiLEqDryVMcfyKFStWIHiHQUdSPKiOiwUvdKwWOMoJCQmaBqR1jlodo1XB2a5fvz4pNlTHxcI4dExk3X9El3HRDfg4qmodi2UoDjEWB+ofFwuM9BrTDY8RDfycMmXKEI6B4UwENDEKSDSE2mMTZd68eRg3LmrGhUFg57sOGTKEaA7VsemCbt/Dhw8xMEn4D40fmy6IXXh5eQ0aNIhw0uPqvwAAAh1JREFUAIw1YsCcaAu1x6YOXFI7OztXV1cD+hgXLlzw8fGRz6jWAqpjinTG8OXLlxFj9vf3J/yE+hUU6dz55s2b7969G9FlUrKcO3euwDRu7aD2mPKBmJiYlJQU+URqfRMdHX3x4kUtptEVhtpjygfc3d3R+QsMDCQlgoeHh05ETKiOKQVgn4J+/Pixvhvqzp07Kz4p85FQv4KihKysrCdPniCOofiAtw755Zdf2rRpU9Sj4FpAdUwpkr59+/7444/Ozs6E81AdU1SB6DJCyzqcIvfzzz9bWFj079+f6BTqH1NUwY5NLFiwgOgCjNjBuutcxITaY0pxOHXqFHzZBg0aEK5C7TFFPR07dqxYsWJERER8fDzRiri4uGHDhhG9QXVMKRYODg5eXl59+vRRuj6sWn744YcCS93pFupXUDTjypUr/v7+nJq4TKg9pmhK06ZNs7OzN27cWMz8q1atKs4CFB8J1TFFYzA+Ym9vf/nyZbU50UH87LPPGjVqRPQM9SsoWoJuH+LKXFhIgFB7TNEaHx8feMnwlZUuXh8UFLR8+XJSUlAdU7SHYZgbN24ovjKHBTGNY8eOzZ49m5QU1K+g6IbffvtNHwN1xYTaY4puiImJ+eeff7Axa9as6OhoUrJQe0zRGQ8fPrx06VKrVq2qV69OShaqY4oxQNfFohgDVMcUY4DqmGIMUB1TjAGqY4oxQHVMMQaojinGwP8BAAD//8taT+wAAAAGSURBVAMAs3nuWEfI3mEAAAAASUVORK5CYII=",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x000001FD87B261D0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_graph.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6021b124",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_ = graph_ref.State.get_initial_state(article_search='1706.03762')\n",
    "state_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e39361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf4f68f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36m============ get_article_node ============\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34m===== INPUT =====\u001b[0m\u001b[0m\n",
      "1706.03762\n",
      "\u001b[1m\u001b[34m===== OUTPUT =====\u001b[0m\u001b[0m\n",
      "9 chunks Title: Attention Is All You Need\n"
     ]
    }
   ],
   "source": [
    "state_['article'] = nodes.get_article_node(state_)['article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ce60b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36m============ explain_condition ============\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34m===== OUTPUT =====\u001b[0m\u001b[0m\n",
      "explain_chunk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'explain_chunk'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes.explain_condition(state_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "522f9642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32m============ explainer_node : iteration 1 ============\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34m===== INPUT =====\u001b[0m\u001b[0m\n",
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are an article explainer. You will receive an article chunk and the previous explanation you provided.\n",
      "You must provide an explanation that combines the context of the current chunk and the previous explanation until the entire article is explained.\n",
      "The explanation must be in simple words, short and easy to understand.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Chunk 1 of 9:\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗†\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Łukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring significantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "to-German translation task, improving over the existing best results, including\n",
      "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
      "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
      "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\n",
      "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
      "efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
      "our research.\n",
      "†Work performed while at Google Brain.\n",
      "‡Work performed while at Google Research.\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\n",
      "arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\n",
      "1\n",
      "Introduction\n",
      "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\n",
      "in particular, have been firmly established as state of the art approaches in sequence modeling and\n",
      "transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\n",
      "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
      "architectures [38, 24, 15].\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output\n",
      "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\n",
      "states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\n",
      "sequential nature precludes parallelization within training examples, which becomes critical at longer\n",
      "sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\n",
      "significant improvements in computational efficiency through factorization tricks [21] and conditional\n",
      "computation [32], while also improving model performance in case of the latter. The fundamental\n",
      "constraint of sequential computation, however, remains.\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs\n",
      "\u001b[1m\u001b[34m===== OUTPUT =====\u001b[0m\u001b[0m\n",
      "Here's a combined explanation of the current chunk and the previous one:\n",
      "\n",
      "The article discusses a new model architecture called the Transformer, which is designed to perform sequence modeling and transduction tasks. The authors propose this new model based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
      "\n",
      "In simple terms, the Transformer model is an improvement over existing models that use recurrent neural networks (RNNs) or convolutional neural networks. These traditional models have limitations, such as requiring parallelization within training examples and being computationally expensive to train.\n",
      "\n",
      "The authors highlight the importance of attention mechanisms in sequence modeling and transduction tasks. Attention allows a model to focus on specific parts of the input data that are relevant for making predictions. However, most existing models use attention in conjunction with RNNs or convolutions.\n",
      "\n",
      "In contrast, the Transformer model relies entirely on attention mechanisms to draw global dependencies between input and output. This approach enables the model to be more parallelizable and can lead to significant improvements in translation quality.\n",
      "\n",
      "The authors report impressive results for their proposed model, achieving state-of-the-art performance on two machine translation tasks: English-to-German and English-to-French. They also demonstrate that the Transformer generalizes well to other tasks, such as English constituency parsing.\n",
      "\n",
      "Overall, the Transformer model represents a significant departure from traditional sequence modeling architectures and has the potential to revolutionize the field of natural language processing.\n"
     ]
    }
   ],
   "source": [
    "res=nodes.explainer_node(state_)\n",
    "state_['current_explanation'] = res['current_explanation']\n",
    "state_['current_doc'] = res['current_doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "423d0644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a combined explanation of the current chunk and the previous one:\n",
      "\n",
      "The article discusses a new model architecture called the Transformer, which is designed to perform sequence modeling and transduction tasks. The authors propose this new model based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
      "\n",
      "In simple terms, the Transformer model is an improvement over existing models that use recurrent neural networks (RNNs) or convolutional neural networks. These traditional models have limitations, such as requiring parallelization within training examples and being computationally expensive to train.\n",
      "\n",
      "The authors highlight the importance of attention mechanisms in sequence modeling and transduction tasks. Attention allows a model to focus on specific parts of the input data that are relevant for making predictions. However, most existing models use attention in conjunction with RNNs or convolutions.\n",
      "\n",
      "In contrast, the Transformer model relies entirely on attention mechanisms to draw global dependencies between input and output. This approach enables the model to be more parallelizable and can lead to significant improvements in translation quality.\n",
      "\n",
      "The authors report impressive results for their proposed model, achieving state-of-the-art performance on two machine translation tasks: English-to-German and English-to-French. They also demonstrate that the Transformer generalizes well to other tasks, such as English constituency parsing.\n",
      "\n",
      "Overall, the Transformer model represents a significant departure from traditional sequence modeling architectures and has the potential to revolutionize the field of natural language processing.\n"
     ]
    }
   ],
   "source": [
    "print(state_['current_explanation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54b26e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_['current_doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a04d0500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32m============ explainer_node : iteration 2 ============\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34m===== INPUT =====\u001b[0m\u001b[0m\n",
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are an article explainer. You will receive an article chunk and the previous explanation you provided.\n",
      "You must provide an explanation that combines the context of the current chunk and the previous explanation until the entire article is explained.\n",
      "The explanation must be in simple words, short and easy to understand.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here's a combined explanation of the current chunk and the previous one:\n",
      "\n",
      "The article discusses a new model architecture called the Transformer, which is designed to perform sequence modeling and transduction tasks. The authors propose this new model based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
      "\n",
      "In simple terms, the Transformer model is an improvement over existing models that use recurrent neural networks (RNNs) or convolutional neural networks. These traditional models have limitations, such as requiring parallelization within training examples and being computationally expensive to train.\n",
      "\n",
      "The authors highlight the importance of attention mechanisms in sequence modeling and transduction tasks. Attention allows a model to focus on specific parts of the input data that are relevant for making predictions. However, most existing models use attention in conjunction with RNNs or convolutions.\n",
      "\n",
      "In contrast, the Transformer model relies entirely on attention mechanisms to draw global dependencies between input and output. This approach enables the model to be more parallelizable and can lead to significant improvements in translation quality.\n",
      "\n",
      "The authors report impressive results for their proposed model, achieving state-of-the-art performance on two machine translation tasks: English-to-German and English-to-French. They also demonstrate that the Transformer generalizes well to other tasks, such as English constituency parsing.\n",
      "\n",
      "Overall, the Transformer model represents a significant departure from traditional sequence modeling architectures and has the potential to revolutionize the field of natural language processing.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Chunk 2 of 9:\n",
      ".\n",
      "The Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2\n",
      "Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\n",
      "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
      "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
      "it more difficult to learn dependencies between distant positions [12]. In the Transformer this is\n",
      "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
      "textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34].\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "3\n",
      "Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\n",
      "Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\n",
      "of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\n",
      "sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n",
      "[10], consuming the previously generated symbols as additional input when generating the next.\n",
      "2\n",
      "Figure 1: The Transformer - model architecture.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1\n",
      "Encoder and Decoder Stacks\n",
      "Encoder:\n",
      "The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
      "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "wise fully connected feed-forward network. We employ a residual connection [11] around each of\n",
      "the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\n",
      "LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\n",
      "itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
      "layers, produce outputs of dimension dmodel = 512.\n",
      "Decoder:\n",
      "The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\n",
      "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\n",
      "attention over the output of the encoder stack. Similar to the encoder, we employ residual connections\n",
      "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
      "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
      "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
      "predictions for position i can depend only on the known outputs at positions less than i.\n",
      "3.2\n",
      "Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "3\n",
      "Scaled Dot-Product Attention\n",
      "Multi-Head Attention\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "attention layers running in parallel.\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
      "query with the corresponding key\n",
      "\u001b[1m\u001b[34m===== OUTPUT =====\u001b[0m\u001b[0m\n",
      "Here's a combined explanation of the current chunk and the previous one:\n",
      "\n",
      "The article discusses a new model architecture called the Transformer, which is designed to perform sequence modeling and transduction tasks. The authors propose this new model based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
      "\n",
      "In simple terms, the Transformer model is an improvement over existing models that use recurrent neural networks (RNNs) or convolutional neural networks. These traditional models have limitations, such as requiring parallelization within training examples and being computationally expensive to train.\n",
      "\n",
      "The authors highlight the importance of attention mechanisms in sequence modeling and transduction tasks. Attention allows a model to focus on specific parts of the input data that are relevant for making predictions. However, most existing models use attention in conjunction with RNNs or convolutions.\n",
      "\n",
      "In contrast, the Transformer model relies entirely on attention mechanisms to draw global dependencies between input and output. This approach enables the model to be more parallelizable and can lead to significant improvements in translation quality.\n",
      "\n",
      "The authors also explain that traditional sequence modeling architectures, such as those using convolutional neural networks, have limitations due to the distance between positions in the input sequence. As a result, it becomes increasingly difficult to learn dependencies between distant positions.\n",
      "\n",
      "To address this issue, the Transformer model uses self-attention mechanisms, which allow it to focus on specific parts of the input data regardless of their position. This is achieved by using scaled dot-product attention and multi-head attention.\n",
      "\n",
      "Multi-head attention consists of several attention layers running in parallel, each with a different set of weights and biases. This allows the model to capture multiple aspects of the input data simultaneously.\n",
      "\n",
      "The Transformer model also uses point-wise, fully connected layers for both the encoder and decoder, as well as residual connections and layer normalization to facilitate these connections.\n",
      "\n",
      "Here's a key difference between the Transformer model and traditional sequence modeling architectures: the Transformer does not use recurrence or convolutional neural networks. Instead, it relies solely on attention mechanisms to compute representations of its input and output.\n",
      "\n",
      "This approach enables the model to be more parallelizable and can lead to significant improvements in translation quality. The authors report impressive results for their proposed model, achieving state-of-the-art performance on two machine translation tasks: English-to-German and English-to-French.\n",
      "\n",
      "Overall, the Transformer model represents a significant departure from traditional sequence modeling architectures and has the potential to revolutionize the field of natural language processing.\n"
     ]
    }
   ],
   "source": [
    "res=nodes.explainer_node(state_)\n",
    "state_['current_explanation'] = res['current_explanation']\n",
    "state_['current_doc'] = res['current_doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2ea833e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a combined explanation of the current chunk and the previous one:\n",
      "\n",
      "The article discusses a new model architecture called the Transformer, which is designed to perform sequence modeling and transduction tasks. The authors propose this new model based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
      "\n",
      "In simple terms, the Transformer model is an improvement over existing models that use recurrent neural networks (RNNs) or convolutional neural networks. These traditional models have limitations, such as requiring parallelization within training examples and being computationally expensive to train.\n",
      "\n",
      "The authors highlight the importance of attention mechanisms in sequence modeling and transduction tasks. Attention allows a model to focus on specific parts of the input data that are relevant for making predictions. However, most existing models use attention in conjunction with RNNs or convolutions.\n",
      "\n",
      "In contrast, the Transformer model relies entirely on attention mechanisms to draw global dependencies between input and output. This approach enables the model to be more parallelizable and can lead to significant improvements in translation quality.\n",
      "\n",
      "The authors also explain that traditional sequence modeling architectures, such as those using convolutional neural networks, have limitations due to the distance between positions in the input sequence. As a result, it becomes increasingly difficult to learn dependencies between distant positions.\n",
      "\n",
      "To address this issue, the Transformer model uses self-attention mechanisms, which allow it to focus on specific parts of the input data regardless of their position. This is achieved by using scaled dot-product attention and multi-head attention.\n",
      "\n",
      "Multi-head attention consists of several attention layers running in parallel, each with a different set of weights and biases. This allows the model to capture multiple aspects of the input data simultaneously.\n",
      "\n",
      "The Transformer model also uses point-wise, fully connected layers for both the encoder and decoder, as well as residual connections and layer normalization to facilitate these connections.\n",
      "\n",
      "Here's a key difference between the Transformer model and traditional sequence modeling architectures: the Transformer does not use recurrence or convolutional neural networks. Instead, it relies solely on attention mechanisms to compute representations of its input and output.\n",
      "\n",
      "This approach enables the model to be more parallelizable and can lead to significant improvements in translation quality. The authors report impressive results for their proposed model, achieving state-of-the-art performance on two machine translation tasks: English-to-German and English-to-French.\n",
      "\n",
      "Overall, the Transformer model represents a significant departure from traditional sequence modeling architectures and has the potential to revolutionize the field of natural language processing.\n"
     ]
    }
   ],
   "source": [
    "print(state_['current_explanation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fdf1e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_['current_doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3699996",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_web_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
