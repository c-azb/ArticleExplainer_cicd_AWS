{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "019c1857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed61a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.post('http://127.0.0.1:8000/user/register',json={'username':'Carlos','password':'123'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "740c667e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detail': [{'type': 'missing',\n",
       "   'loc': ['body', 'confirm_password'],\n",
       "   'msg': 'Field required',\n",
       "   'input': {'username': 'Carlos', 'password': '123'}}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a431c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.post('http://127.0.0.1:8000/user/login',data={'username':'Carlos','password':'123'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3895eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RequestsCookieJar[Cookie(version=0, name='refresh_token', value='eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJDYXJsb3MiLCJ0eXBlIjoicmVmcmVzaCIsImV4cCI6MTc2MDAxODM3M30.HPJizSqjTKpkzfYL3YDR-O79pJE-H98sQ_NUKVL1wH0', port=None, port_specified=False, domain='127.0.0.1', domain_specified=False, domain_initial_dot=False, path='/', path_specified=True, secure=True, expires=1760018373, discard=False, comment=None, comment_url=None, rest={'HttpOnly': None, 'SameSite': 'none'}, rfc2109=False)]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.cookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bba0000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'access_token': 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJDYXJsb3MiLCJleHAiOjE3NTk5MzMzMTF9.oyeTmGoXEa8HJl8Z71pIuFPFqrC4mRMYFeE6qVxh-b8'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b730638",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = res.json()['access_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cddf08ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.post('http://127.0.0.1:8000/graph',\n",
    "                    json={'search_query':'1706.03762'},\n",
    "                    headers={\"Authorization\": f\"Bearer {access_token}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cba54048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b79639d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'explanation': \"Here's a combined explanation of the article chunk 2 and 3, incorporating context from previous explanations:\\n\\n**The Transformer Architecture: A Breakthrough in Sequence Modeling**\\n\\nIn our previous explanation, we introduced the Transformer architecture, which represents a breakthrough in sequence modeling. This approach to sequence modeling is different from traditional recurrent neural networks (RNNs) and convolutional neural networks.\\n\\n**Why Traditional Models Have Limitations**\\n\\nTraditional models have limitations in terms of computation time and parallelization. RNNs rely on recurrence, which makes it challenging to process long sequences in parallel. Conventional models also use attention mechanisms, but these are often used in conjunction with traditional recurrent architectures.\\n\\n**The Transformer: A New Approach to Sequence Modeling**\\n\\nThe Transformer architecture is designed to overcome the limitations of traditional models. It relies entirely on an attention mechanism to draw global dependencies between input and output sequences. This approach allows for more parallelization and can reach state-of-the-art performance in sequence modeling tasks, such as machine translation.\\n\\n**The Benefits of Self-Attention**\\n\\nIn the Transformer model, self-attention is used to relate different positions of a single sequence. This is also known as intra-attention. Self-attention has been successfully used in various tasks, including reading comprehension, abstractive summarization, and learning task-independent sentence representations. However, traditional models like Extended Neural GPU, ByteNet, and ConvS2S use convolutional neural networks as the basic building block, which makes it more difficult to learn dependencies between distant positions.\\n\\n**The Transformer's Advantage**\\n\\nIn the Transformer model, self-attention is used to compute representations of its input and output without using sequence-aligned RNNs or convolution. This approach allows for parallelization and can reach state-of-the-art performance in sequence modeling tasks with much less training time.\\n\\n**Model Architecture: The Encoder-Decoder Structure**\\n\\nThe Transformer follows an encoder-decoder structure, which is commonly used in competitive neural sequence transduction models. The encoder maps the input sequence of symbol representations to a sequence of continuous representations, while the decoder generates the output sequence one element at a time.\\n\\n**The Stacked Self-Attention Mechanism**\\n\\nIn both the encoder and decoder stacks, self-attention is employed as a multi-head attention mechanism, followed by a position-wise fully connected feed-forward network. The multi-head attention mechanism allows for parallelization of the computation and can help to reduce the dimensionality of the input data.\\n\\n**The Decoder's Additional Layer: Multi-Head Attention**\\n\\nIn the decoder stack, an additional layer is added that performs multi-head attention over the output of the encoder stack. This ensures that the predictions for each position can depend only on the known outputs at positions less than i.\\n\\n**The Self-Attention Mechanism: A Scaled Dot-Product Attention**\\n\\nSelf-attention can be described as mapping a query and a set of key-value pairs to an output, where the output is computed as a weighted sum of the values. The weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\\n\\nIn summary, the Transformer architecture represents a breakthrough in sequence modeling, offering improved performance, parallelization, and reduced training time compared to traditional models. By using self-attention, the model can compute representations of its input and output without using sequence-aligned RNNs or convolution, making it more efficient and effective.\",\n",
       " 'title': 'Attention Is All You Need'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "10253831",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('http://127.0.0.1:8000/graph/explanations',\n",
    "                    headers={\"Authorization\": f\"Bearer {access_token}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5b33426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': '68dfcee588bb53f078827203',\n",
       "  'title': 'Attention Is All You Need',\n",
       "  'explanation': \"Here's a combined explanation of the article chunk 2 and 3, incorporating context from previous explanations:\\n\\n**The Transformer Architecture: A Breakthrough in Sequence Modeling**\\n\\nIn our previous explanation, we introduced the Transformer architecture, which represents a breakthrough in sequence modeling. This approach to sequence modeling is different from traditional recurrent neural networks (RNNs) and convolutional neural networks.\\n\\n**Why Traditional Models Have Limitations**\\n\\nTraditional models have limitations in terms of computation time and parallelization. RNNs rely on recurrence, which makes it challenging to process long sequences in parallel. Conventional models also use attention mechanisms, but these are often used in conjunction with traditional recurrent architectures.\\n\\n**The Transformer: A New Approach to Sequence Modeling**\\n\\nThe Transformer architecture is designed to overcome the limitations of traditional models. It relies entirely on an attention mechanism to draw global dependencies between input and output sequences. This approach allows for more parallelization and can reach state-of-the-art performance in sequence modeling tasks, such as machine translation.\\n\\n**The Benefits of Self-Attention**\\n\\nIn the Transformer model, self-attention is used to relate different positions of a single sequence. This is also known as intra-attention. Self-attention has been successfully used in various tasks, including reading comprehension, abstractive summarization, and learning task-independent sentence representations. However, traditional models like Extended Neural GPU, ByteNet, and ConvS2S use convolutional neural networks as the basic building block, which makes it more difficult to learn dependencies between distant positions.\\n\\n**The Transformer's Advantage**\\n\\nIn the Transformer model, self-attention is used to compute representations of its input and output without using sequence-aligned RNNs or convolution. This approach allows for parallelization and can reach state-of-the-art performance in sequence modeling tasks with much less training time.\\n\\n**Model Architecture: The Encoder-Decoder Structure**\\n\\nThe Transformer follows an encoder-decoder structure, which is commonly used in competitive neural sequence transduction models. The encoder maps the input sequence of symbol representations to a sequence of continuous representations, while the decoder generates the output sequence one element at a time.\\n\\n**The Stacked Self-Attention Mechanism**\\n\\nIn both the encoder and decoder stacks, self-attention is employed as a multi-head attention mechanism, followed by a position-wise fully connected feed-forward network. The multi-head attention mechanism allows for parallelization of the computation and can help to reduce the dimensionality of the input data.\\n\\n**The Decoder's Additional Layer: Multi-Head Attention**\\n\\nIn the decoder stack, an additional layer is added that performs multi-head attention over the output of the encoder stack. This ensures that the predictions for each position can depend only on the known outputs at positions less than i.\\n\\n**The Self-Attention Mechanism: A Scaled Dot-Product Attention**\\n\\nSelf-attention can be described as mapping a query and a set of key-value pairs to an output, where the output is computed as a weighted sum of the values. The weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\\n\\nIn summary, the Transformer architecture represents a breakthrough in sequence modeling, offering improved performance, parallelization, and reduced training time compared to traditional models. By using self-attention, the model can compute representations of its input and output without using sequence-aligned RNNs or convolution, making it more efficient and effective.\"}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5ed9aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('http://127.0.0.1:8000/graph/explanation',\n",
    "                   params={'id':'68dfcee588bb53f078827203'},\n",
    "                    headers={\"Authorization\": f\"Bearer {access_token}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45f61577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': '68dfcee588bb53f078827203',\n",
       " 'title': 'Attention Is All You Need',\n",
       " 'explanation': \"Here's a combined explanation of the article chunk 2 and 3, incorporating context from previous explanations:\\n\\n**The Transformer Architecture: A Breakthrough in Sequence Modeling**\\n\\nIn our previous explanation, we introduced the Transformer architecture, which represents a breakthrough in sequence modeling. This approach to sequence modeling is different from traditional recurrent neural networks (RNNs) and convolutional neural networks.\\n\\n**Why Traditional Models Have Limitations**\\n\\nTraditional models have limitations in terms of computation time and parallelization. RNNs rely on recurrence, which makes it challenging to process long sequences in parallel. Conventional models also use attention mechanisms, but these are often used in conjunction with traditional recurrent architectures.\\n\\n**The Transformer: A New Approach to Sequence Modeling**\\n\\nThe Transformer architecture is designed to overcome the limitations of traditional models. It relies entirely on an attention mechanism to draw global dependencies between input and output sequences. This approach allows for more parallelization and can reach state-of-the-art performance in sequence modeling tasks, such as machine translation.\\n\\n**The Benefits of Self-Attention**\\n\\nIn the Transformer model, self-attention is used to relate different positions of a single sequence. This is also known as intra-attention. Self-attention has been successfully used in various tasks, including reading comprehension, abstractive summarization, and learning task-independent sentence representations. However, traditional models like Extended Neural GPU, ByteNet, and ConvS2S use convolutional neural networks as the basic building block, which makes it more difficult to learn dependencies between distant positions.\\n\\n**The Transformer's Advantage**\\n\\nIn the Transformer model, self-attention is used to compute representations of its input and output without using sequence-aligned RNNs or convolution. This approach allows for parallelization and can reach state-of-the-art performance in sequence modeling tasks with much less training time.\\n\\n**Model Architecture: The Encoder-Decoder Structure**\\n\\nThe Transformer follows an encoder-decoder structure, which is commonly used in competitive neural sequence transduction models. The encoder maps the input sequence of symbol representations to a sequence of continuous representations, while the decoder generates the output sequence one element at a time.\\n\\n**The Stacked Self-Attention Mechanism**\\n\\nIn both the encoder and decoder stacks, self-attention is employed as a multi-head attention mechanism, followed by a position-wise fully connected feed-forward network. The multi-head attention mechanism allows for parallelization of the computation and can help to reduce the dimensionality of the input data.\\n\\n**The Decoder's Additional Layer: Multi-Head Attention**\\n\\nIn the decoder stack, an additional layer is added that performs multi-head attention over the output of the encoder stack. This ensures that the predictions for each position can depend only on the known outputs at positions less than i.\\n\\n**The Self-Attention Mechanism: A Scaled Dot-Product Attention**\\n\\nSelf-attention can be described as mapping a query and a set of key-value pairs to an output, where the output is computed as a weighted sum of the values. The weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\\n\\nIn summary, the Transformer architecture represents a breakthrough in sequence modeling, offering improved performance, parallelization, and reduced training time compared to traditional models. By using self-attention, the model can compute representations of its input and output without using sequence-aligned RNNs or convolution, making it more efficient and effective.\"}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c54aae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.delete('http://127.0.0.1:8000/graph/explanation',\n",
    "                   params={'id':'68dfcee588bb53f078827203'},\n",
    "                    headers={\"Authorization\": f\"Bearer {access_token}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a98694e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f13b105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abcd2171",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.post('http://127.0.0.1:8000/graph',json={'search_query':'1706.03762'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8463da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OK'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d8d3f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"explanation\":\"**Explaining the Article: Variants of the Transformer Model**\\\\n===============\\\\n\\\\n**Introduction**\\\\n---------------\\\\n\\\\nThe Transformer model is a popular sequence-to-sequence model used in natural language processing tasks. It consists of a multi-head attention mechanism and a feed-forward network (FFN) layer. The FFN layer is a crucial component of the Transformer model, and in this article, we explore variants of the FFN layer using Gated Linear Units (GLU) and its variants.\\\\n\\\\n**GLU and Its Variants**\\\\n----------------------\\\\n\\\\nGLU is a neural network layer defined as the component-wise product of two linear transformations of the input, one of which is sigmoid-activated. Variants of GLU can be defined using different activation functions. Some of these variants include:\\\\n\\\\n* ReLU-GLU: uses the ReLU activation function\\\\n* GELU-GLU: uses the GELU activation function\\\\n* Swish-GLU: uses the Swish activation function\\\\n\\\\n**FFN Variants**\\\\n----------------\\\\n\\\\nThe authors propose variants of the FFN layer that use GLU or its variants in place of the first linear transformation and the activation function. These variants include:\\\\n\\\\n* FFN-GLU: uses GLU in place of the first linear transformation\\\\n* FFN-Bilinear: uses the bilinear layer in place of the first linear transformation\\\\n* FFN-ReGLU: uses the ReLU-GLU variant in place of the first linear transformation\\\\n* FFN-GELU: uses the GELU-GLU variant in place of the first linear transformation\\\\n* FFN-Swish: uses the Swish-GLU variant in place of the first linear transformation\\\\n\\\\n**Experiments**\\\\n----------------\\\\n\\\\nThe authors test the FFN variants on the text-to-text transfer transformer (T5) model. They use the same codebase, model architecture, and training task as the base model from [Raﬀel et al., 2019]. The results show that some of the FFN variants perform better than the baseline model on the heldout-set log-perplexity task.\\\\n\\\\n**Results**\\\\n------------\\\\n\\\\nThe results of the experiments are listed in the tables below. The GEGLU and SwiGLU variants produce the best perplexities.\\\\n\\\\n**Pre-training Results**\\\\n----------------------\\\\n\\\\n| Model | Heldout-set log-perplexity |\\\\n| --- | --- |\\\\n| FFN-ReLU (baseline) | 2.000 (0.005) |\\\\n| FFN-GELU | 1.983 (0.005) |\\\\n| FFN-Swish | 1.994 (0.003) |\\\\n| FFN-GLU | 1.982 (0.006) |\\\\n| FFN-GEGLU | 1.942 (0.004) |\\\\n| FFN-SwiGLU | 1.944 (0.010) |\\\\n| FFN-ReGLU | 1.953 (0.003) |\\\\n\\\\n**Fine-tuning Results**\\\\n----------------------\\\\n\\\\nThe authors fine-tune each fully-trained model once on an example s-proportional mixture of the Stanford Question-Answering Dataset (SQuAD) [Rajpurkar et al., 2016] and all the language understanding tasks in the GLUE [Wang et al., 2018] and SuperGlue [Wang et al., 2019] benchmarks. The results on the development sets are listed in the tables below.\\\\n\\\\n**GLUE Benchmark**\\\\n------------------\\\\n\\\\n| Model | CoLA | SST-2 | MRPC | STSB | QQP | MNLIm | QNLI | RTE |\\\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\\\\n| FFN-ReLU | 83.80 | 51.32 | 94.04 | 93.08 | 90.20 | 89.64 | 89.42 | 89.01 |\\\\n| FFN-GELU | 83.86 | 53.48 | 94.04 | 92.81 | 90.20 | 89.69 | 89.49 | 88.63 |\\\\n| FFN-Swish | 83.60 | 49.79 | 93.69 | 92.31 | 89.46 | 89.20 | 88.98 | 88.84 |\\\\n| FFN-GLU | 84.20 | 49.16 | 94.27 | 92.39 | 89.46 | 89.46 | 89.35 | 88.79 |\\\\n| FFN-GEGLU | 84.12 | 53.65 | 93.92 | 92.68 | 89.71 | 90.26 | 90.13 | 89.11 |\\\\n| FFN-SwiGLU | 84.36 | 51.59 | 93.92 | 92.23 | 88.97 | 90.32 | 90.13 | 89.14 |\\\\n| FFN-ReGLU | 84.67 | 56.16 | 94.38 | 92.06 | 89.22 | 89.97 | 89.85 | 88.86 |\\\\n| [Raﬀel et al., 2019] | 83.28 | 53.84 | 92.68 | 92.07 | 88.92 | 88.02 | 87.94 | 88.67 |\\\\n\\\\n**SuperGlue Benchmark**\\\\n----------------------\\\\n\\\\n| Model | CoLA | SST-2 | MRPC | STSB | QQP | MNLIm | QNLI | RTE |\\\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\\\\n| FFN-ReLU | 83.80 | 51.32 | 94.04 | 93.08 | 90.20 | 89.64 | 89.42 | 89.01 |\\\\n| FFN-GELU | 83.86 | 53.48 | 94.04 | 92.81 | 90.20 | 89.69 | 89.49 | 88.63 |\\\\n| FFN-Swish | 83.60 | 49.79 | 93.69 | 92.31 | 89.46 | 89.20 | 88.98 | 88.84 |\\\\n| FFN-GLU | 84.20 | 49.16 | 94.27 | 92.39 | 89.46 | 89.46 | 89.35 | 88.79 |\\\\n| FFN-GEGLU | 84.12 | 53.65 | 93.92 | 92.68 | 89.71 | 90.26 | 90.13 | 89.11 |\\\\n| FFN-SwiGLU | 84.36 | 51.59 | 93.92 | 92.23 | 88.97 | 90.32 | 90.13 | 89.14 |\\\\n| FFN-ReGLU | 84.67 | 56.16 | 94.38 | 92.06 | 89.22 | 89.97 | 89.85 | 88.86 |\\\\n| [Raﬀel et al., 2019] | 83.28 | 53.84 | 92.68 | 92.07 | 88.92 | 88.02 | 87.94 | 88.67 |\\\\n\\\\n**Conclusion**\\\\n----------\\\\n\\\\nIn this article, we explored variants of the FFN layer using GLU and its variants. The results show that some of the FFN variants perform better than the baseline model on the heldout-set log-perplexity task and the downstream language understanding tasks. The best performing model is the FFN-GEGLU model, which achieves the best perplexities on the pre-training task and the best results on the GLUE and SuperGlue benchmarks. The authors attribute the success of these architectures to divine benevolence.\",\"title\":\"\",\"_id\":\"0\"}'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c00fd345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fff7bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Explaining the Article: Variants of the Transformer Model**\n",
      "===============\n",
      "\n",
      "**Introduction**\n",
      "---------------\n",
      "\n",
      "The Transformer model is a popular sequence-to-sequence model used in natural language processing tasks. It consists of a multi-head attention mechanism and a feed-forward network (FFN) layer. The FFN layer is a crucial component of the Transformer model, and in this article, we explore variants of the FFN layer using Gated Linear Units (GLU) and its variants.\n",
      "\n",
      "**GLU and Its Variants**\n",
      "----------------------\n",
      "\n",
      "GLU is a neural network layer defined as the component-wise product of two linear transformations of the input, one of which is sigmoid-activated. Variants of GLU can be defined using different activation functions. Some of these variants include:\n",
      "\n",
      "* ReLU-GLU: uses the ReLU activation function\n",
      "* GELU-GLU: uses the GELU activation function\n",
      "* Swish-GLU: uses the Swish activation function\n",
      "\n",
      "**FFN Variants**\n",
      "----------------\n",
      "\n",
      "The authors propose variants of the FFN layer that use GLU or its variants in place of the first linear transformation and the activation function. These variants include:\n",
      "\n",
      "* FFN-GLU: uses GLU in place of the first linear transformation\n",
      "* FFN-Bilinear: uses the bilinear layer in place of the first linear transformation\n",
      "* FFN-ReGLU: uses the ReLU-GLU variant in place of the first linear transformation\n",
      "* FFN-GELU: uses the GELU-GLU variant in place of the first linear transformation\n",
      "* FFN-Swish: uses the Swish-GLU variant in place of the first linear transformation\n",
      "\n",
      "**Experiments**\n",
      "----------------\n",
      "\n",
      "The authors test the FFN variants on the text-to-text transfer transformer (T5) model. They use the same codebase, model architecture, and training task as the base model from [Raﬀel et al., 2019]. The results show that some of the FFN variants perform better than the baseline model on the heldout-set log-perplexity task.\n",
      "\n",
      "**Results**\n",
      "------------\n",
      "\n",
      "The results of the experiments are listed in the tables below. The GEGLU and SwiGLU variants produce the best perplexities.\n",
      "\n",
      "**Pre-training Results**\n",
      "----------------------\n",
      "\n",
      "| Model | Heldout-set log-perplexity |\n",
      "| --- | --- |\n",
      "| FFN-ReLU (baseline) | 2.000 (0.005) |\n",
      "| FFN-GELU | 1.983 (0.005) |\n",
      "| FFN-Swish | 1.994 (0.003) |\n",
      "| FFN-GLU | 1.982 (0.006) |\n",
      "| FFN-GEGLU | 1.942 (0.004) |\n",
      "| FFN-SwiGLU | 1.944 (0.010) |\n",
      "| FFN-ReGLU | 1.953 (0.003) |\n",
      "\n",
      "**Fine-tuning Results**\n",
      "----------------------\n",
      "\n",
      "The authors fine-tune each fully-trained model once on an example s-proportional mixture of the Stanford Question-Answering Dataset (SQuAD) [Rajpurkar et al., 2016] and all the language understanding tasks in the GLUE [Wang et al., 2018] and SuperGlue [Wang et al., 2019] benchmarks. The results on the development sets are listed in the tables below.\n",
      "\n",
      "**GLUE Benchmark**\n",
      "------------------\n",
      "\n",
      "| Model | CoLA | SST-2 | MRPC | STSB | QQP | MNLIm | QNLI | RTE |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "| FFN-ReLU | 83.80 | 51.32 | 94.04 | 93.08 | 90.20 | 89.64 | 89.42 | 89.01 |\n",
      "| FFN-GELU | 83.86 | 53.48 | 94.04 | 92.81 | 90.20 | 89.69 | 89.49 | 88.63 |\n",
      "| FFN-Swish | 83.60 | 49.79 | 93.69 | 92.31 | 89.46 | 89.20 | 88.98 | 88.84 |\n",
      "| FFN-GLU | 84.20 | 49.16 | 94.27 | 92.39 | 89.46 | 89.46 | 89.35 | 88.79 |\n",
      "| FFN-GEGLU | 84.12 | 53.65 | 93.92 | 92.68 | 89.71 | 90.26 | 90.13 | 89.11 |\n",
      "| FFN-SwiGLU | 84.36 | 51.59 | 93.92 | 92.23 | 88.97 | 90.32 | 90.13 | 89.14 |\n",
      "| FFN-ReGLU | 84.67 | 56.16 | 94.38 | 92.06 | 89.22 | 89.97 | 89.85 | 88.86 |\n",
      "| [Raﬀel et al., 2019] | 83.28 | 53.84 | 92.68 | 92.07 | 88.92 | 88.02 | 87.94 | 88.67 |\n",
      "\n",
      "**SuperGlue Benchmark**\n",
      "----------------------\n",
      "\n",
      "| Model | CoLA | SST-2 | MRPC | STSB | QQP | MNLIm | QNLI | RTE |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "| FFN-ReLU | 83.80 | 51.32 | 94.04 | 93.08 | 90.20 | 89.64 | 89.42 | 89.01 |\n",
      "| FFN-GELU | 83.86 | 53.48 | 94.04 | 92.81 | 90.20 | 89.69 | 89.49 | 88.63 |\n",
      "| FFN-Swish | 83.60 | 49.79 | 93.69 | 92.31 | 89.46 | 89.20 | 88.98 | 88.84 |\n",
      "| FFN-GLU | 84.20 | 49.16 | 94.27 | 92.39 | 89.46 | 89.46 | 89.35 | 88.79 |\n",
      "| FFN-GEGLU | 84.12 | 53.65 | 93.92 | 92.68 | 89.71 | 90.26 | 90.13 | 89.11 |\n",
      "| FFN-SwiGLU | 84.36 | 51.59 | 93.92 | 92.23 | 88.97 | 90.32 | 90.13 | 89.14 |\n",
      "| FFN-ReGLU | 84.67 | 56.16 | 94.38 | 92.06 | 89.22 | 89.97 | 89.85 | 88.86 |\n",
      "| [Raﬀel et al., 2019] | 83.28 | 53.84 | 92.68 | 92.07 | 88.92 | 88.02 | 87.94 | 88.67 |\n",
      "\n",
      "**Conclusion**\n",
      "----------\n",
      "\n",
      "In this article, we explored variants of the FFN layer using GLU and its variants. The results show that some of the FFN variants perform better than the baseline model on the heldout-set log-perplexity task and the downstream language understanding tasks. The best performing model is the FFN-GEGLU model, which achieves the best perplexities on the pre-training task and the best results on the GLUE and SuperGlue benchmarks. The authors attribute the success of these architectures to divine benevolence.\n"
     ]
    }
   ],
   "source": [
    "print(res.json()['explanation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b8a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ebff66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d43b3d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2674a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34636b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_web_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
