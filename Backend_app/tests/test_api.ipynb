{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "019c1857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed61a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.post('http://127.0.0.1:8000/user/register',json={'username':'Carlos','password':'123'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "740c667e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detail': [{'type': 'missing',\n",
       "   'loc': ['body', 'confirm_password'],\n",
       "   'msg': 'Field required',\n",
       "   'input': {'username': 'Carlos', 'password': '123'}}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a431c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.post('http://127.0.0.1:8000/user/login',data={'username':'Carlos','password':'123'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3895eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RequestsCookieJar[Cookie(version=0, name='refresh_token', value='eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJDYXJsb3MiLCJ0eXBlIjoicmVmcmVzaCIsImV4cCI6MTc2MDAxODM3M30.HPJizSqjTKpkzfYL3YDR-O79pJE-H98sQ_NUKVL1wH0', port=None, port_specified=False, domain='127.0.0.1', domain_specified=False, domain_initial_dot=False, path='/', path_specified=True, secure=True, expires=1760018373, discard=False, comment=None, comment_url=None, rest={'HttpOnly': None, 'SameSite': 'none'}, rfc2109=False)]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.cookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bba0000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'access_token': 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJDYXJsb3MiLCJleHAiOjE3NTk5MzMzMTF9.oyeTmGoXEa8HJl8Z71pIuFPFqrC4mRMYFeE6qVxh-b8'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b730638",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = res.json()['access_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cddf08ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.post('http://127.0.0.1:8000/graph',\n",
    "                    json={'search_query':'1706.03762'},\n",
    "                    headers={\"Authorization\": f\"Bearer {access_token}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cba54048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b79639d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'explanation': \"Here's a combined explanation of the article chunk 2 and 3, incorporating context from previous explanations:\\n\\n**The Transformer Architecture: A Breakthrough in Sequence Modeling**\\n\\nIn our previous explanation, we introduced the Transformer architecture, which represents a breakthrough in sequence modeling. This approach to sequence modeling is different from traditional recurrent neural networks (RNNs) and convolutional neural networks.\\n\\n**Why Traditional Models Have Limitations**\\n\\nTraditional models have limitations in terms of computation time and parallelization. RNNs rely on recurrence, which makes it challenging to process long sequences in parallel. Conventional models also use attention mechanisms, but these are often used in conjunction with traditional recurrent architectures.\\n\\n**The Transformer: A New Approach to Sequence Modeling**\\n\\nThe Transformer architecture is designed to overcome the limitations of traditional models. It relies entirely on an attention mechanism to draw global dependencies between input and output sequences. This approach allows for more parallelization and can reach state-of-the-art performance in sequence modeling tasks, such as machine translation.\\n\\n**The Benefits of Self-Attention**\\n\\nIn the Transformer model, self-attention is used to relate different positions of a single sequence. This is also known as intra-attention. Self-attention has been successfully used in various tasks, including reading comprehension, abstractive summarization, and learning task-independent sentence representations. However, traditional models like Extended Neural GPU, ByteNet, and ConvS2S use convolutional neural networks as the basic building block, which makes it more difficult to learn dependencies between distant positions.\\n\\n**The Transformer's Advantage**\\n\\nIn the Transformer model, self-attention is used to compute representations of its input and output without using sequence-aligned RNNs or convolution. This approach allows for parallelization and can reach state-of-the-art performance in sequence modeling tasks with much less training time.\\n\\n**Model Architecture: The Encoder-Decoder Structure**\\n\\nThe Transformer follows an encoder-decoder structure, which is commonly used in competitive neural sequence transduction models. The encoder maps the input sequence of symbol representations to a sequence of continuous representations, while the decoder generates the output sequence one element at a time.\\n\\n**The Stacked Self-Attention Mechanism**\\n\\nIn both the encoder and decoder stacks, self-attention is employed as a multi-head attention mechanism, followed by a position-wise fully connected feed-forward network. The multi-head attention mechanism allows for parallelization of the computation and can help to reduce the dimensionality of the input data.\\n\\n**The Decoder's Additional Layer: Multi-Head Attention**\\n\\nIn the decoder stack, an additional layer is added that performs multi-head attention over the output of the encoder stack. This ensures that the predictions for each position can depend only on the known outputs at positions less than i.\\n\\n**The Self-Attention Mechanism: A Scaled Dot-Product Attention**\\n\\nSelf-attention can be described as mapping a query and a set of key-value pairs to an output, where the output is computed as a weighted sum of the values. The weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\\n\\nIn summary, the Transformer architecture represents a breakthrough in sequence modeling, offering improved performance, parallelization, and reduced training time compared to traditional models. By using self-attention, the model can compute representations of its input and output without using sequence-aligned RNNs or convolution, making it more efficient and effective.\",\n",
       " 'title': 'Attention Is All You Need'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "10253831",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('http://127.0.0.1:8000/graph/explanations',\n",
    "                    headers={\"Authorization\": f\"Bearer {access_token}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5b33426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': '68dfcee588bb53f078827203',\n",
       "  'title': 'Attention Is All You Need',\n",
       "  'explanation': \"Here's a combined explanation of the article chunk 2 and 3, incorporating context from previous explanations:\\n\\n**The Transformer Architecture: A Breakthrough in Sequence Modeling**\\n\\nIn our previous explanation, we introduced the Transformer architecture, which represents a breakthrough in sequence modeling. This approach to sequence modeling is different from traditional recurrent neural networks (RNNs) and convolutional neural networks.\\n\\n**Why Traditional Models Have Limitations**\\n\\nTraditional models have limitations in terms of computation time and parallelization. RNNs rely on recurrence, which makes it challenging to process long sequences in parallel. Conventional models also use attention mechanisms, but these are often used in conjunction with traditional recurrent architectures.\\n\\n**The Transformer: A New Approach to Sequence Modeling**\\n\\nThe Transformer architecture is designed to overcome the limitations of traditional models. It relies entirely on an attention mechanism to draw global dependencies between input and output sequences. This approach allows for more parallelization and can reach state-of-the-art performance in sequence modeling tasks, such as machine translation.\\n\\n**The Benefits of Self-Attention**\\n\\nIn the Transformer model, self-attention is used to relate different positions of a single sequence. This is also known as intra-attention. Self-attention has been successfully used in various tasks, including reading comprehension, abstractive summarization, and learning task-independent sentence representations. However, traditional models like Extended Neural GPU, ByteNet, and ConvS2S use convolutional neural networks as the basic building block, which makes it more difficult to learn dependencies between distant positions.\\n\\n**The Transformer's Advantage**\\n\\nIn the Transformer model, self-attention is used to compute representations of its input and output without using sequence-aligned RNNs or convolution. This approach allows for parallelization and can reach state-of-the-art performance in sequence modeling tasks with much less training time.\\n\\n**Model Architecture: The Encoder-Decoder Structure**\\n\\nThe Transformer follows an encoder-decoder structure, which is commonly used in competitive neural sequence transduction models. The encoder maps the input sequence of symbol representations to a sequence of continuous representations, while the decoder generates the output sequence one element at a time.\\n\\n**The Stacked Self-Attention Mechanism**\\n\\nIn both the encoder and decoder stacks, self-attention is employed as a multi-head attention mechanism, followed by a position-wise fully connected feed-forward network. The multi-head attention mechanism allows for parallelization of the computation and can help to reduce the dimensionality of the input data.\\n\\n**The Decoder's Additional Layer: Multi-Head Attention**\\n\\nIn the decoder stack, an additional layer is added that performs multi-head attention over the output of the encoder stack. This ensures that the predictions for each position can depend only on the known outputs at positions less than i.\\n\\n**The Self-Attention Mechanism: A Scaled Dot-Product Attention**\\n\\nSelf-attention can be described as mapping a query and a set of key-value pairs to an output, where the output is computed as a weighted sum of the values. The weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\\n\\nIn summary, the Transformer architecture represents a breakthrough in sequence modeling, offering improved performance, parallelization, and reduced training time compared to traditional models. By using self-attention, the model can compute representations of its input and output without using sequence-aligned RNNs or convolution, making it more efficient and effective.\"}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5ed9aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('http://127.0.0.1:8000/graph/explanation',\n",
    "                   params={'id':'68dfcee588bb53f078827203'},\n",
    "                    headers={\"Authorization\": f\"Bearer {access_token}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45f61577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': '68dfcee588bb53f078827203',\n",
       " 'title': 'Attention Is All You Need',\n",
       " 'explanation': \"Here's a combined explanation of the article chunk 2 and 3, incorporating context from previous explanations:\\n\\n**The Transformer Architecture: A Breakthrough in Sequence Modeling**\\n\\nIn our previous explanation, we introduced the Transformer architecture, which represents a breakthrough in sequence modeling. This approach to sequence modeling is different from traditional recurrent neural networks (RNNs) and convolutional neural networks.\\n\\n**Why Traditional Models Have Limitations**\\n\\nTraditional models have limitations in terms of computation time and parallelization. RNNs rely on recurrence, which makes it challenging to process long sequences in parallel. Conventional models also use attention mechanisms, but these are often used in conjunction with traditional recurrent architectures.\\n\\n**The Transformer: A New Approach to Sequence Modeling**\\n\\nThe Transformer architecture is designed to overcome the limitations of traditional models. It relies entirely on an attention mechanism to draw global dependencies between input and output sequences. This approach allows for more parallelization and can reach state-of-the-art performance in sequence modeling tasks, such as machine translation.\\n\\n**The Benefits of Self-Attention**\\n\\nIn the Transformer model, self-attention is used to relate different positions of a single sequence. This is also known as intra-attention. Self-attention has been successfully used in various tasks, including reading comprehension, abstractive summarization, and learning task-independent sentence representations. However, traditional models like Extended Neural GPU, ByteNet, and ConvS2S use convolutional neural networks as the basic building block, which makes it more difficult to learn dependencies between distant positions.\\n\\n**The Transformer's Advantage**\\n\\nIn the Transformer model, self-attention is used to compute representations of its input and output without using sequence-aligned RNNs or convolution. This approach allows for parallelization and can reach state-of-the-art performance in sequence modeling tasks with much less training time.\\n\\n**Model Architecture: The Encoder-Decoder Structure**\\n\\nThe Transformer follows an encoder-decoder structure, which is commonly used in competitive neural sequence transduction models. The encoder maps the input sequence of symbol representations to a sequence of continuous representations, while the decoder generates the output sequence one element at a time.\\n\\n**The Stacked Self-Attention Mechanism**\\n\\nIn both the encoder and decoder stacks, self-attention is employed as a multi-head attention mechanism, followed by a position-wise fully connected feed-forward network. The multi-head attention mechanism allows for parallelization of the computation and can help to reduce the dimensionality of the input data.\\n\\n**The Decoder's Additional Layer: Multi-Head Attention**\\n\\nIn the decoder stack, an additional layer is added that performs multi-head attention over the output of the encoder stack. This ensures that the predictions for each position can depend only on the known outputs at positions less than i.\\n\\n**The Self-Attention Mechanism: A Scaled Dot-Product Attention**\\n\\nSelf-attention can be described as mapping a query and a set of key-value pairs to an output, where the output is computed as a weighted sum of the values. The weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\\n\\nIn summary, the Transformer architecture represents a breakthrough in sequence modeling, offering improved performance, parallelization, and reduced training time compared to traditional models. By using self-attention, the model can compute representations of its input and output without using sequence-aligned RNNs or convolution, making it more efficient and effective.\"}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c54aae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.delete('http://127.0.0.1:8000/graph/explanation',\n",
    "                   params={'id':'68dfcee588bb53f078827203'},\n",
    "                    headers={\"Authorization\": f\"Bearer {access_token}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a98694e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f13b105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abcd2171",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.post('https://klmtayegstmenwbhdrclace6340tkqfl.lambda-url.sa-east-1.on.aws//graph',json={'search_query':'1706.03762'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8463da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bad Gateway'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d8d3f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Internal Server Error'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c00fd345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "502"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff7bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.json()['explanation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b8a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ebff66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d43b3d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2674a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34636b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_web_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
